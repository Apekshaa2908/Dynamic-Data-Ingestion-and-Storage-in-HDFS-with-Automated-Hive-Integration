{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVXhT/aOPnDKRe3yVynqn8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apekshaa2908/Dynamic-Data-Ingestion-and-Storage-in-HDFS-with-Automated-Hive-Integration/blob/main/Dynamic_Data_Ingestion_and_Storage_in_HDFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Download and Install Hadoop and Hive\n",
        "\n"
      ],
      "metadata": {
        "id": "QQsM8Xne4sxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w1n_bY_4aTo"
      },
      "outputs": [],
      "source": [
        "#Hive installation\n",
        "wget https://downloads.apache.org/hive/hive-4.0.0/apache-hive-4.0.0-bin.tar.gz\n",
        "\n",
        "#Hadoop installation\n",
        "wget https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Download the Data File"
      ],
      "metadata": {
        "id": "dvi7mo7M41_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Census data CSV file\n",
        "wget https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/cities/totals/sub-est2023_32.csv\n"
      ],
      "metadata": {
        "id": "dmUjHDka41pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " HDFS Setup"
      ],
      "metadata": {
        "id": "yPlN9-ly4-Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List contents of /user directory in HDFS\n",
        "hadoop fs -ls /user\n",
        "\n",
        "# Create necessary directories in HDFS\n",
        "hadoop fs -mkdir /user/hadoop\n",
        "hadoop fs -mkdir /user/hadoop/my_directory\n",
        "\n",
        "# Upload the CSV file to HDFS\n",
        "hadoop fs -put sub-est2023_32.csv /user/hadoop/my_directory/\n"
      ],
      "metadata": {
        "id": "QRx2MJ8v4-2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Hive Setup and Data Loading"
      ],
      "metadata": {
        "id": "EZc-oanr5Awe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Hive CLI\n",
        "hive\n",
        "\n",
        "# Show existing databases\n",
        "SHOW DATABASES;\n",
        "\n",
        "# Create a new database\n",
        "CREATE DATABASE myproject;\n",
        "\n",
        "# Use the newly created database\n",
        "USE myproject;\n",
        "\n",
        "# Drop the table if it already exists\n",
        "DROP TABLE IF EXISTS my_table;\n",
        "\n",
        "# Create the table with the specified schema\n",
        "CREATE TABLE my_table (\n",
        "    SUMLEV INT,\n",
        "    STATE INT,\n",
        "    COUNTY INT,\n",
        "    PLACE INT,\n",
        "    COUSUB INT,\n",
        "    CONCIT INT,\n",
        "    PRIMGEO_FLAG INT,\n",
        "    FUNCSTAT STRING,\n",
        "    NAME STRING,\n",
        "    STNAME STRING,\n",
        "    ESTIMATESBASE2020 INT,\n",
        "    POPESTIMATE2020 INT,\n",
        "    POPESTIMATE2021 INT,\n",
        "    POPESTIMATE2022 INT,\n",
        "    POPESTIMATE2023 INT,\n",
        "    PRIMARY KEY (STATE, COUNTY, PLACE, NAME) DISABLE NOVALIDATE\n",
        ")\n",
        "ROW FORMAT DELIMITED\n",
        "FIELDS TERMINATED BY ','\n",
        "STORED AS TEXTFILE;\n",
        "\n",
        "# Load data into the Hive table from HDFS\n",
        "LOAD DATA INPATH '/user/hadoop/my_directory/sub-est2023_32.csv' INTO TABLE my_table;\n",
        "\n",
        "# Query the table to verify data load\n",
        "SELECT * FROM my_table LIMIT 10;\n"
      ],
      "metadata": {
        "id": "QKACTIHo5GRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nano hive_automation.sh"
      ],
      "metadata": {
        "id": "u_Q_iwFpdKkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Define variables\n",
        "HDFS_DIR=\"/user/hadoop/my_directory\"\n",
        "LOCAL_FILE=\"sub-est2023_32.csv\"\n",
        "HDFS_PATH=\"$HDFS_DIR/$LOCAL_FILE\"\n",
        "HIVE_DB=\"myproject\"\n",
        "HIVE_TABLE=\"my_table\"\n",
        "SOURCE_URL=\"https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/cities/totals/$LOCAL_FILE\"\n",
        "\n",
        "# 1. Download the CSV file using wget\n",
        "wget $SOURCE_URL -O $LOCAL_FILE\n",
        "\n",
        "# 2. Check if the HDFS directory exists, create if not\n",
        "hadoop fs -test -d $HDFS_DIR\n",
        "if [ $? -ne 0 ]; then\n",
        "  echo \"Creating HDFS directory: $HDFS_DIR\"\n",
        "  hadoop fs -mkdir -p $HDFS_DIR\n",
        "fi\n",
        "\n",
        "# 3. Upload the CSV file to HDFS\n",
        "echo \"Uploading $LOCAL_FILE to HDFS: $HDFS_PATH\"\n",
        "hadoop fs -put -f $LOCAL_FILE $HDFS_PATH\n",
        "\n",
        "# 4. Start Hive CLI and execute the Hive commands\n",
        "echo \"Starting Hive process\"\n",
        "hive -e \"\n",
        "  -- Create database if it does not exist\n",
        "  CREATE DATABASE IF NOT EXISTS $HIVE_DB;\n",
        "\n",
        "  -- Use the database\n",
        "  USE $HIVE_DB;\n",
        "\n",
        "  -- Drop the table if it already exists\n",
        "  DROP TABLE IF EXISTS $HIVE_TABLE;\n",
        "\n",
        "  -- Create the table with schema matching the CSV file\n",
        "  CREATE TABLE $HIVE_TABLE (\n",
        "      SUMLEV INT,\n",
        "      STATE INT,\n",
        "      COUNTY INT,\n",
        "      PLACE INT,\n",
        "      COUSUB INT,\n",
        "      CONCIT INT,\n",
        "      PRIMGEO_FLAG INT,\n",
        "      FUNCSTAT STRING,\n",
        "      NAME STRING,\n",
        "      STNAME STRING,\n",
        "      ESTIMATESBASE2020 INT,\n",
        "      POPESTIMATE2020 INT,\n",
        "      POPESTIMATE2021 INT,\n",
        "      POPESTIMATE2022 INT,\n",
        "      POPESTIMATE2023 INT\n",
        "  )\n",
        "  ROW FORMAT DELIMITED\n",
        "  FIELDS TERMINATED BY ','\n",
        "  STORED AS TEXTFILE;\n",
        "\n",
        "  -- Load data into the Hive table from HDFS\n",
        "  LOAD DATA INPATH '$HDFS_PATH' INTO TABLE $HIVE_TABLE;\n",
        "\n",
        "  -- Query the table to verify data load\n",
        "  SELECT * FROM $HIVE_TABLE LIMIT 10;\n",
        "\"\n",
        "\n",
        "# 5. Clean up local file if needed\n",
        "echo \"Cleaning up local CSV file\"\n",
        "rm $LOCAL_FILE\n",
        "\n",
        "echo \"Automation script completed successfully.\"\n"
      ],
      "metadata": {
        "id": "B2XC3tcqdUEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chmod +x hive_automation.sh\n",
        "./hive_automation.sh"
      ],
      "metadata": {
        "id": "XW5YMrRrdcWS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}